{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "from autogen import register_function, AssistantAgent, UserProxyAgent\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent import QdrantRetrieveUserProxyAgent\n",
    "from autogen.agentchat.contrib.web_surfer import WebSurferAgent\n",
    "from qdrant_client import QdrantClient\n",
    "from ARGO import ArgoWrapper\n",
    "from CustomLLMAutogen2 import ARGO_LLM\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All available models\n",
    "config_list = [\n",
    "        {\n",
    "            \"model\": \"gemini-pro\",\n",
    "            \"api_key\": os.environ[\"GOOGLE_API_KEY\"],\n",
    "            \"api_type\": \"google\",\n",
    "            \"tags\": [\"gemini\", \"api\"]\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"phi3\", \n",
    "            \"api_key\": \"ollama\", \n",
    "            \"base_url\": 'http://localhost:11434/v1',\n",
    "            \"tags\": [\"ollama\", \"phi\"]\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"llama3\", \n",
    "            \"api_key\": \"ollama\", \n",
    "            \"base_url\": 'http://localhost:11434/v1',\n",
    "            \"tags\": [\"ollama\", \"llama\"]\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"mistral\", \n",
    "            \"api_key\": \"ollama\", \n",
    "            \"base_url\": 'http://localhost:11434/v1',\n",
    "            \"tags\": [\"ollama\", \"mistral\"]\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"gemma:7b\", \n",
    "            \"api_key\": \"ollama\", \n",
    "            \"base_url\": 'http://localhost:11434/v1',\n",
    "            \"tags\": [\"ollama\", \"gemma\"]\n",
    "        },\n",
    "        {\n",
    "            'model': 'gpt-3.5-turbo-16k',\n",
    "            'api_key': os.environ[\"OPENAI_API_KEY\"],\n",
    "            'tags': ['gpt3.5']\n",
    "        },\n",
    "        {\n",
    "            'model': 'Argo',\n",
    "            'api_type': 'argo',\n",
    "            'argo_client': ARGO_LLM(argo=ArgoWrapper,model_type='gpt4', temperature = 0.3),\n",
    "            'tags': ['argo']\n",
    "        },\n",
    "]\n",
    "# Filters the models based on the tags. Filters models\n",
    "filter_dict = {'tags': ['gpt3.5']}\n",
    "config_list = autogen.filter_config(config_list, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up configuration for agents\n",
    "llm_config = {\n",
    "    \"config_list\": config_list, \n",
    "    \"cache_seed\": None, # Ensures differing responses\n",
    "    \"timeout\": 600,\n",
    "    \"seed\": 42,\n",
    "    \"temperature\": 1, # Temperature max is 2\n",
    "}\n",
    "llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "googleai_embedding_function= embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key = os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\r\", \"\\t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA = {\n",
    "  \"question\": \"The generated question\",\n",
    "  \"correct_answer\": \"The correct answer\",\n",
    "  \"distractors\": [\n",
    "    \"Incorrect answer #1\", \"Incorrect answer #2\", \"Incorrect Answer #3\", \"Incorrect Answer #4\"\n",
    "  ],\n",
    "  \"skills\": '''Choose the necessary skills for answering the question using at least one of the following options \n",
    "  [Generalization, Basic comprehension, Summarization, Interpolation/extrapolation, Cross-domain application, Reasoning, General knowledge, Fundamental domain science concepts, Understanding identifiers/notation, Understanding evolution of ideas]''',\n",
    "  \"domains\": '''Choose the most applicable domain of the question using at least one of the following options \n",
    "  [physics, material science, biology, chemistry, computer science, mathematics, climate]''',\n",
    "  \"difficulty\": \"Choose the difficulty of the question using one of the following options ['easy', 'medium', 'hard'] \",\n",
    "  \"doi\": \"Identify the digital object identifier (DOI) of the paper and provide it here. It will be the link of the form doi.org\",\n",
    "  \"author\": \"Jose A. Tandoc\",\n",
    "  \"comments\": \"generated question\",\n",
    "  \"affiliation\": \"Argonne\",\n",
    "  \"position\": \"Student\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = f'''Generate 3 unique 5 choice multiple choice question that are extremely difficult. Use the paper as context, but do not directly reference the paper in the question.\n",
    "These questions should be general knowledge, such that the question statement cannot have 'paper', 'experiment', or 'study'.\n",
    "All context required to answer the question must be provided within the question statement.\n",
    "This question MUST be answerable without reading the paper.\n",
    "There should be exactly one correct answer.\n",
    "The incorrect answers must be difficult to distinguish from the correct answer, however they cannot be correct under the context of the paper.\n",
    "The incorrect answers are 'distractors' that are designed to be confuse the large language model that is answering the question.\n",
    "'''\n",
    "\n",
    "doc = r\"C:\\Users\\judoc\\Documents\\Work\\Argonne\\papers\\Mechanism Design for Large Language Models.pdf\"\n",
    "collection = 'mechDesign3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_assistant = QdrantRetrieveUserProxyAgent(\n",
    "    name=\"assistant\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    default_auto_reply=\"Reply `TERMINATE` if the task is done.\",\n",
    "    # max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": [\n",
    "            doc,\n",
    "        ],\n",
    "        \"custom_text_split_function\": text_splitter.split_text,\n",
    "        \"embedding_funcion\": googleai_embedding_function,\n",
    "        \"client\": QdrantClient(url=\"https://7f9bbc68-cbea-48e0-9841-2dd23f878d28.us-east4-0.gcp.cloud.qdrant.io\", api_key= os.environ[\"QDRANT_API_KEY\"]),\n",
    "        \"collection_name\": collection,\n",
    "        \"get_or_create\": True,\n",
    "    },\n",
    "    code_execution_config=False,\n",
    "    description=\"Assistant who has extra content retrieval power for solving difficult problems.\",\n",
    ")\n",
    "\n",
    "expert = AssistantAgent(\n",
    "    \"Expert\",\n",
    "    system_message=f'''You are an expert on {problem} Assist in answering the problem. Then, put the information in a list using the following schema: {SCHEMA}. Follow the instructions of the SCHEMA.\n",
    "    You should return a list: [SCHEMA, SCHEMA, SCHEMA].\n",
    "    Reply 'TERMINATE' in the end when everything is done.''',\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    description=\"Expert in question generation.\",\n",
    ")\n",
    "\n",
    "generality_verifier = autogen.ConversableAgent(\n",
    "    \"generality_verifier\",\n",
    "    system_message=f'''You are a generality verifier that ensures that the question is general enough to be answered without reading the paper.\n",
    "    If the question mentions 'in the paper' or 'in this study', then it is too specific and you prompt the expert to formulate new questions.\n",
    "    Reply 'TERMINATE' in the end when everything is done.''',\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    description=\"Generality Verifier who ensures that each question is general enough to be answered without reading the given paper.\",\n",
    ")\n",
    "\n",
    "# verifier = autogen.ConversableAgent(\n",
    "#     \"Verifier\",\n",
    "#     system_message=f'''You are a verifier that uses the provided context for {problem} to ensure that the expert has concluded a correct answer. \n",
    "#     Reply 'VALID' if the question and answers are correct and if the format is correct.\n",
    "#     Otherwise, reply 'INVALID' and provide feedback on how to improve the question and answers.''',\n",
    "#     llm_config=llm_config,\n",
    "#     human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "#     description=\"Verifier who can verify the correctness of expert's questions.\",\n",
    "# )\n",
    "automator = AssistantAgent(\n",
    "    'question_generation_automator',\n",
    "    system_message=f'''You are an automator that takes the generated question and pipes it into a pandas dataframe using the add_question function.\n",
    "    Reply 'TERMINATE' in the end when everything is done.''',\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    description=\"Automator utilizes functions to automate the process of generating questions.\",\n",
    ")\n",
    "\n",
    "user_proxy = autogen.ConversableAgent(\n",
    "    name=\"User\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    description=\"The proxy that adds the question to the CSV file using add_question function\"\n",
    ")\n",
    "\n",
    "# web_surfer = WebSurferAgent(\n",
    "#     \"web_surfer\",\n",
    "#     system_message=f'''You are a web surfer that can find the DOI of a paper using the paper's title.\n",
    "#     Reply 'TERMINATE' in the end when everything is done''',\n",
    "#     llm_config=llm_config,\n",
    "#     summarizer_llm_config=llm_config,\n",
    "#     browser_config={\"viewport_size\": 4096, \"bing_api_key\": os.environ[\"BING_API_KEY\"]},\n",
    "#     description=\"Web Surfer who can search the web for information.\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_question(question: str, correct_answer: str, distractors: List[str], skills: str, domains: str, difficulty: str, doi: str, author: str, comments: str, affiliation: str, position: str) -> str:\n",
    "    cur = pd.DataFrame([{\n",
    "        \"question\": question,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"distractors\": distractors,\n",
    "        \"skills\": skills,\n",
    "        \"domains\": domains,\n",
    "        \"difficulty\": difficulty,\n",
    "        \"doi\": doi,\n",
    "        \"author\": author,\n",
    "        \"comments\": comments,\n",
    "        \"affiliation\": affiliation,\n",
    "        \"position\": position,\n",
    "    }])\n",
    "    if os.path.exists(r\"generatedQuestions/generated_questions.csv\"):\n",
    "        df = pd.read_csv(r\"generatedQuestions/generated_questions.csv\")\n",
    "        df = pd.concat([df, cur])\n",
    "        df.to_csv(r\"generatedQuestions/generated_questions.csv\", index=False)\n",
    "    else:\n",
    "        cur.to_csv(r\"generatedQuestions/generated_questions.csv\", index=False)\n",
    "    return \"Question added to the CSV file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_questions(questions: List[Dict]) -> str:\n",
    "    cur = None\n",
    "    for question in questions:\n",
    "        if cur == None:\n",
    "            cur = pd.DataFrame([question])\n",
    "        else:\n",
    "            cur = pd.concat([df, pd.DataFrame([question])])\n",
    "    if os.path.exists(r\"generatedQuestions/generated_questions.csv\"):\n",
    "        df = pd.read_csv(r\"generatedQuestions/generated_questions.csv\")\n",
    "        df = pd.concat([df, cur])\n",
    "        df.to_csv(r\"generatedQuestions/generated_questions.csv\", index=False)\n",
    "    else:\n",
    "        cur.to_csv(r\"generatedQuestions/generated_questions.csv\", index=False)\n",
    "    return 'Finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_function(\n",
    "    add_question,\n",
    "    caller=automator,\n",
    "    executor=user_proxy,\n",
    "    name=\"add_question\",\n",
    "    description=\"Adds all generated questions to the CSV file.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset_agents():\n",
    "    retrieval_assistant.reset()\n",
    "    automator.reset()\n",
    "    generality_verifier.reset()\n",
    "    expert.reset()\n",
    "    user_proxy.reset()\n",
    "    #verifier.reset()\n",
    "    #web_surfer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_transitions = {\n",
    "    retrieval_assistant: [expert],\n",
    "    expert: [generality_verifier],\n",
    "    generality_verifier: [expert, automator],\n",
    "    #verifier: [user_proxy, expert],\n",
    "    automator: [user_proxy],\n",
    "    user_proxy: [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat():\n",
    "    _reset_agents()\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[retrieval_assistant, expert, automator, user_proxy], messages=[], max_round=10,\n",
    "        #allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "        #speaker_transitions_type=\"allowed\",\n",
    "        speaker_selection_method=\"round_robin\",\n",
    "        send_introductions=True, # Provides information on each agent in the group chat to the manager.\n",
    "    )\n",
    "    \n",
    "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={\n",
    "        \"config_list\": config_list, \n",
    "        \"cache_seed\": None, # Ensures differing responses\n",
    "        \"timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Start chatting with boss_aid as this is the user proxy agent.\n",
    "    retrieval_assistant.initiate_chat(\n",
    "        manager,\n",
    "        message=retrieval_assistant.message_generator,\n",
    "        problem=problem,\n",
    "        n_results=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy = autogen.ConversableAgent(\n",
    "#     name=\"User\",\n",
    "#     llm_config=False,\n",
    "#     is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "#     human_input_mode=\"NEVER\",\n",
    "# )\n",
    "\n",
    "# assistant = autogen.ConversableAgent(\n",
    "#     name=\"Assistant\",\n",
    "#     system_message=\"You are a helpful AI assistant. \"\n",
    "#     \"You help with JSON Formatting\"\n",
    "#     \"Return 'TERMINATE' when the task is done.\",\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "# test = []\n",
    "# def json_return(first_name: str, last_name: str, email: str) -> Dict:\n",
    "#     form = {\n",
    "#         'first_name': first_name,\n",
    "#         'last_name': last_name,\n",
    "#         'email': email,\n",
    "#     }\n",
    "#     test.append(form)\n",
    "#     return form\n",
    "\n",
    "# register_function(\n",
    "#     json_return,\n",
    "#     caller=assistant,  # The assistant agent can suggest calls to the calculator.\n",
    "#     executor=user_proxy,  # The user proxy agent can execute the calculator calls.\n",
    "#     name=\"json_return\",  # By default, the function name is used as the tool name.\n",
    "#     description=\"A json object returner\",  # A description of the tool.\n",
    "# )\n",
    "# form = {\n",
    "#     'first_name': 'first name',\n",
    "#     'last_name': 'last name',\n",
    "#     'email': 'email',\n",
    "# }\n",
    "# chat_result = user_proxy.initiate_chat(assistant, message= f'''Produce a json object using the following format: {form} given this information: Alec Tandoc, metandoc@gmail.com''', max_turns=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
