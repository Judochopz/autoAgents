{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A jupyter notebook that generates questions based on provided texts and submits them to the Aurora API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "from autogen import register_function, AssistantAgent, UserProxyAgent, ConversableAgent\n",
    "from autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent import QdrantRetrieveUserProxyAgent\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from ARGO import ArgoWrapper\n",
    "from CustomLLMAutogen2 import ARGO_LLM\n",
    "from typing import Dict, List, Any, Optional\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required:\n",
    "# OPEN_AI_API_KEY\n",
    "# GOOGLE_API_KEY, for free, found in Google AI Studio: https://ai.google.dev/gemini-api\n",
    "# ID, must be obtained from the Aurora API, found here https://web.cels.anl.gov/projects/auroragptquestions/docs#/default/store_author_api_author_post \n",
    "# Optional: \n",
    "# QDRANT_URL\n",
    "# QDRANT_API_KEY\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All available models\n",
    "config_list = [\n",
    "        {\n",
    "            'model': 'gpt-3.5-turbo-16k',\n",
    "            'tags': ['gpt3.5']\n",
    "        },\n",
    "        {\n",
    "            'model': 'Argo',\n",
    "            'api_type': 'argo',\n",
    "            'argo_client': ARGO_LLM(argo=ArgoWrapper,model_type='gpt4', temperature = 0.3),\n",
    "            'tags': ['argo']\n",
    "        },\n",
    "]\n",
    "# Filters the models based on the tags. Filters models\n",
    "filter_dict = {'tags': ['gpt3.5']}\n",
    "config_list = autogen.filter_config(config_list, filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up configuration for agents\n",
    "llm_config = {\n",
    "    \"config_list\": config_list, \n",
    "    \"cache_seed\": None, # Ensures differing responses\n",
    "    \"timeout\": 600,\n",
    "    \"seed\": 44,\n",
    "    \"temperature\": 0.2, # Temperature max is 2\n",
    "}\n",
    "# GPT4 for better question generation\n",
    "llm_config_gen = {\n",
    "    \"config_list\": \n",
    "    [\n",
    "        {\n",
    "            'model': 'gpt-4-turbo',\n",
    "        }\n",
    "    ], \n",
    "    \"cache_seed\": None, # Ensures differing responses\n",
    "    \"timeout\": 600,\n",
    "    \"seed\": 44,\n",
    "    \"temperature\": 1.0, # Temperature max is 2\n",
    "    \"response_format\": { \"type\": \"json_object\" },\n",
    "}\n",
    "llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary embedding function for the vector database\n",
    "googleai_embedding_function= embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key = os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks text for vectorization\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\r\", \"\\t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF EDITED, change add_questions function\n",
    "QUESTION_COUNT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = f'''For each paper, generate {QUESTION_COUNT} unique and extremely difficult to answer multiple choice questions with 5 choices each.\n",
    "The answerer does not have access to the paper, you cannot require context for the question. Do not include author names, dates, or any other identifying information.\n",
    "These should be general knowledge questions with supporting evidence from the paper.\n",
    "All context required to answer the question must be provided within the question statement.\n",
    "The question statement cannot include 'in this study', 'in this paper', 'according to the paper', etc.\n",
    "There should be exactly one correct answer.\n",
    "The incorrect answers must be difficult to distinguish from the correct answer, however they cannot be correct.\n",
    "The incorrect answers are 'distractors' that are designed to be confuse the large language model that is answering the question.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema(filename: str):\n",
    "    return {\n",
    "  \"question\": \"The generated question\",\n",
    "  \"correct_answer\": \"The correct answer\",\n",
    "  \"distractors\": [\n",
    "    \"Incorrect answer #1\", \"Incorrect answer #2\", \"Incorrect Answer #3\", \"Incorrect Answer #4\"\n",
    "  ],\n",
    "  \"skills\": f'''Choose the necessary skills for answering the question using at least one of the following options \n",
    "  {[\"Generalization\", \"Basic comprehension\", \"Summarization\", \"Interpolation/extrapolation\", \"Cross-domain application\", \"Reasoning\", \"General knowledge\", \"Fundamental domain science concepts\", \"Understanding identifiers/notation\", \"Understanding evolution of ideas\"]}\n",
    "  Must be in the form of a {list}''',\n",
    "  \"domains\": f'''Choose the most applicable domains for the question using at least one of the following options\n",
    "  {[\"physics\", \"material science\", \"biology\", \"chemistry\", \"computer science\", \"mathematics\", \"climate\"]}\n",
    "  Must be in the form of a {list}''',\n",
    "  \"difficulty\": \"Choose the difficulty of the question using one of the following options ['easy', 'medium', 'hard'] \",\n",
    "  \"doi\": filename,\n",
    "  \"author\": \"BLANK\",\n",
    "  \"support\": \"NA\",\n",
    "  \"comments\": \"generated question\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_ENTRIES = [\n",
    "        \"question\",\n",
    "        \"correct_answer\",\n",
    "        \"distractors\",\n",
    "        \"skills\",\n",
    "        \"domains\",\n",
    "        \"difficulty\",\n",
    "        \"doi\",\n",
    "        \"author\",\n",
    "        \"support\",\n",
    "        \"comments\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_RAG_and_expert(docs: List[str], collection: str, filename: str):\n",
    "    \"\"\"\n",
    "    Initiates the RAG and expert agent based on the current file we are generating on.\n",
    "    \"\"\"\n",
    "    retrieval_assistant = QdrantRetrieveUserProxyAgent(\n",
    "        name=\"assistant\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        default_auto_reply=\"Reply `TERMINATE` if the task is done.\",\n",
    "        # max_consecutive_auto_reply=10,\n",
    "        retrieve_config={\n",
    "            \"task\": \"qa\",\n",
    "            \"docs_path\": docs,\n",
    "            \"custom_text_split_function\": text_splitter.split_text,\n",
    "            \"embedding_funcion\": googleai_embedding_function,\n",
    "            \"client\": QdrantClient(\":memory:\"),\n",
    "            # \"client\": QdrantClient(os.environ[\"QDRANT_URL\"], os.environ[\"QDRANT_API_KEY\"]),\n",
    "            # Memory option functional for one-time use, otherwise use the commented out line above for persistent storage\n",
    "            \"collection_name\": collection,\n",
    "            \"get_or_create\": True,\n",
    "        },\n",
    "        code_execution_config=False,\n",
    "        description=\"Assistant who has extra content retrieval power for solving difficult problems.\",\n",
    "    )\n",
    "    expert = AssistantAgent(\n",
    "    \"Expert\",\n",
    "    system_message=f'''You are an expert on {problem} Assist in answering the problem. Then, put the information in a list using the following schema: {schema(filename)}. Follow the instructions of the SCHEMA.\n",
    "    You should return a python {Dict}: {{'question1': SCHEMA, 'question2': SCHEMA, 'question3': SCHEMA, 'question4': SCHEMA, 'question5': SCHEMA}}. Do not change the author, question, affiliation, or comments fields. This response must be a JSON object.\n",
    "    Reply 'TERMINATE' in the end when everything is done.''',\n",
    "    llm_config=llm_config_gen,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    description=\"Expert in question generation.\",\n",
    "    )\n",
    "    return retrieval_assistant, expert\n",
    "\n",
    "automator = AssistantAgent(\n",
    "    'question_generation_automator',\n",
    "    system_message=f'''DO NOT UPDATE CONTEXT. You are an automator that takes the JSON object produced by the expert and makes a function call to the add_questions function.\n",
    "    Reply 'TERMINATE' in the end when everything is done.''',\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    description=\"Automator utilizes functions to automate the process of generating questions.\",\n",
    ")\n",
    "\n",
    "user_proxy = autogen.ConversableAgent(\n",
    "    name=\"User\",\n",
    "    system_message=f'''You are a user that can execute the add_questions function to add the generated questions to the CSV file.''',\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    description=\"The proxy that adds the question to the CSV file using the add_questions function\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING = pd.DataFrame(\n",
    "    columns=SCHEMA_ENTRIES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_CSV = r\"generatedQuestions/generated_questions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_questions(question1: Dict, question2: Dict, question3: Dict, question4: Dict, question5: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Adds questions to the WORKING dataframe and saves them in the CSV file.\n",
    "    Agents cannot interpret dynamic arguments like **kwargs, so we have to pass each question individually.\n",
    "    \"\"\"\n",
    "    args = locals()\n",
    "    questions = []\n",
    "    for _, question in args.items():\n",
    "        questions.append(question)\n",
    "    cur = pd.DataFrame(questions)\n",
    "    if os.path.exists(QUESTION_CSV):\n",
    "        cur.to_csv(QUESTION_CSV, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        cur.to_csv(QUESTION_CSV, mode='w', index=False)\n",
    "    global WORKING\n",
    "    WORKING = pd.concat([WORKING, cur], ignore_index=True)\n",
    "    return 'Finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_function(\n",
    "    add_questions,\n",
    "    caller=automator,\n",
    "    executor=user_proxy,\n",
    "    name=\"add_questions\",\n",
    "    description=\"Adds all generated questions to the CSV file. Arguments required: question1=Dict, question2=Dict, question3=Dict, question4=Dict, question5=Dict\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = '../genQuestionPapers/*.pdf'\n",
    "USED_FILE_PATH = '../genQuestionPapers/usedPapers/*.pdf'\n",
    "docs = glob.glob(FILE_PATH)\n",
    "used = glob.glob(USED_FILE_PATH)\n",
    "filenames = [doc[doc.rfind('\\\\')+1:doc.rfind('.pdf')] for doc in docs]\n",
    "used_filenames = [doc[doc.rfind('\\\\')+1:doc.rfind('.pdf')] for doc in used]\n",
    "idx = 0\n",
    "while idx < len(filenames):\n",
    "    if filenames[idx] in used_filenames:\n",
    "        docs.pop(idx)\n",
    "        filenames.pop(idx)\n",
    "    else:\n",
    "        idx += 1\n",
    "doc_count = len(docs)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset_agents(retrieval_assistant, expert):\n",
    "    \"\"\"\n",
    "    Necessary function to reset the agents for the next document.\n",
    "    \"\"\"\n",
    "    retrieval_assistant.reset()\n",
    "    expert.reset()\n",
    "    automator.reset()\n",
    "    user_proxy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, doc in enumerate(docs[:1]):\n",
    "    \"\"\"\n",
    "    Initiates a new groupchat and for each new expert and retrieval assistant.\n",
    "    \"\"\"\n",
    "    retrieval_assistant, expert = initiate_RAG_and_expert([doc], f'doc{idx}', filenames[idx])\n",
    "    def state_transition(last_speaker, groupchat):\n",
    "        \"\"\"\n",
    "        Defines the order of conversation of the groupchat\n",
    "        \"\"\"\n",
    "        messages = groupchat.messages\n",
    "        if last_speaker is retrieval_assistant:\n",
    "            return expert\n",
    "        elif last_speaker is expert:\n",
    "            if \"UPDATE CONTEXT\" in messages[-1][\"content\"]:\n",
    "                return retrieval_assistant\n",
    "            return automator\n",
    "        elif last_speaker is automator:\n",
    "            return user_proxy\n",
    "        elif last_speaker is user_proxy:\n",
    "            return None\n",
    "    def rag_chat(retrieval_assistant):\n",
    "        \"\"\"\n",
    "        Initiates the RAG chat using the previous functions.\n",
    "        \"\"\"\n",
    "        _reset_agents(retrieval_assistant, expert)\n",
    "        groupchat = autogen.GroupChat(\n",
    "            agents=[retrieval_assistant, expert, automator, user_proxy], messages=[], max_round=20,\n",
    "            speaker_selection_method=state_transition,\n",
    "            send_introductions=True, # Provides information on each agent in the group chat to the manager.\n",
    "        )\n",
    "        \n",
    "        manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={\n",
    "            \"config_list\": config_list, \n",
    "            \"cache_seed\": None, # Ensures differing responses\n",
    "            \"timeout\": 600,\n",
    "            \"seed\": 42,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Start chatting with retrieval_assistant as this is the user proxy agent.\n",
    "        retrieval_assistant.initiate_chat(\n",
    "            manager,\n",
    "            message=retrieval_assistant.message_generator,\n",
    "            problem=problem,\n",
    "            n_results=3,\n",
    "        )\n",
    "    rag_chat(retrieval_assistant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_COUNT = WORKING.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING = pd.read_csv(QUESTION_CSV, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = UserProxyAgent(\n",
    "        name=\"user\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        #is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "        max_consecutive_auto_reply=0,\n",
    "        code_execution_config=False\n",
    "    )\n",
    "\n",
    "analyzer = ConversableAgent(\n",
    "    name=\"Analyzer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"\n",
    "    You are an analyzer who is experienced in determining if a question is too specific if it mentions a specific publication, author, or requires context from the origin paper to answer. Provide a step-by-step thought process, and finally reply with\n",
    "    'VALID' \n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# function_caller = UserProxyAgent(\n",
    "#     name=\"function_caller\",\n",
    "#     human_input_mode=\"NEVER\",\n",
    "#     max_consecutive_auto_reply=2,\n",
    "#     code_execution_config=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_function(context_required: bool, index: Optional[int] = None) -> bool:\n",
    "    if context_required:\n",
    "        WORKING.drop(index, inplace=True)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "### Optional function registration mode, malfunctions when used repeatedly by LLM\n",
    "# register_function(\n",
    "#     dataframe_function,\n",
    "#     caller=analyzer,\n",
    "#     executor=function_caller,\n",
    "#     name=\"dataframe_function\",\n",
    "#     description=\"Removes the question from the dataframe. Arguments required: context_required=bool, index=int\",\n",
    "# )\n",
    "\n",
    "# def speaker_selection(last_speaker, groupchat):\n",
    "#     messages = groupchat.messages\n",
    "#     if last_speaker is user:\n",
    "#         return analyzer\n",
    "#     elif last_speaker is analyzer:\n",
    "#         return function_caller\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was taken from Autogen's ReAct demonstration: https://microsoft.github.io/autogen/docs/topics/prompting-and-reasoning/react/\n",
    "# NOTE: this ReAct prompt is adapted from Langchain's ReAct agent: https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/agents/react/agent.py#L79\n",
    "ReAct_prompt = \"\"\"\n",
    "Answer the following questions as best you can. You have access to tools provided.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take\n",
    "Action Input: the input to the action\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "# Define the ReAct prompt message. Assuming a \"question\" field is present in the context\n",
    "\n",
    "\n",
    "def react_prompt_message(sender, recipient, context):\n",
    "    return ReAct_prompt.format(input=context[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "        agents=[user, analyzer], messages=[], max_round=3,\n",
    "        #speaker_selection_method=speaker_selection,\n",
    "    )\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "for i, row in WORKING.iterrows():\n",
    "    \"\"\"\n",
    "    Throws out questions that require context to answer.\n",
    "    \"\"\"\n",
    "    user.reset()\n",
    "    analyzer.reset()\n",
    "    #function_caller.reset()\n",
    "    response = user.initiate_chat(analyzer, \n",
    "                       message=react_prompt_message,\n",
    "                       question=f\"\"\"Analyze if the question can be answered without reading the origin paper and without searching up anything.\n",
    "                       If it requires context, respond FALSITY.\n",
    "                       Otherwise, response MAGICALITY.\n",
    "                       question: {row['question']}\"\n",
    "                       \"\"\", n_results=1, send_introductions=True, max_round=1)\n",
    "    if 'FALSITY' in response.chat_history[-1]['content'].upper():\n",
    "        dataframe_function(context_required=True, index=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_COUNT = WORKING.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(cur: pd.Series) -> None:\n",
    "    \"\"\"\n",
    "    Prepares data for submission to the Aurora API. LLM JSON objects are only strings, so we need to convert them back to their original form.\n",
    "    \"\"\"\n",
    "    data = cur.copy()\n",
    "    if type(data['distractors']) is str:\n",
    "        data['distractors'] = ast.literal_eval(data[\"distractors\"])\n",
    "    if type(data['skills']) is str:\n",
    "        data['skills'] = ast.literal_eval(data[\"skills\"])\n",
    "    if type(data['domains']) is str:\n",
    "        data['domains'] = ast.literal_eval(data[\"domains\"])\n",
    "    data['author'] = int(os.environ[\"ID\"])\n",
    "    data['support'] = \"NA\"\n",
    "    data['comments'] = \"generated question\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING = WORKING[SCHEMA_ENTRIES]\n",
    "FINAL = pd.DataFrame(\n",
    "    columns=SCHEMA_ENTRIES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function tests each question against Mistral7b, Llama2-7b, Llama3-8b. If at least two models fail the question, the question is saved\n",
    "from json import loads, dumps\n",
    "import requests\n",
    "import pandas as pd\n",
    "length = WORKING.shape[0]\n",
    "headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "TEST_URL = 'https://web.cels.anl.gov/projects/auroragptquestions/api/test_question'\n",
    "SUBMIT_URL = 'https://web.cels.anl.gov/projects/auroragptquestions/api/question'\n",
    "for idx in range(length):\n",
    "    cur = WORKING.iloc[idx]\n",
    "    edited_data = prep_data(cur)\n",
    "    if edited_data.isna().any():\n",
    "        continue\n",
    "    data = dumps(edited_data.to_dict())\n",
    "    response = requests.post(TEST_URL, headers=headers, data=data)\n",
    "    if response.status_code == 200:\n",
    "        content = loads(response.text)\n",
    "        correct = 0\n",
    "        for model in content:\n",
    "            if model[\"correct\"]:\n",
    "                correct += 1\n",
    "        if correct <= 1:\n",
    "            FINAL = pd.concat([FINAL, cur.to_frame().T], ignore_index=True)\n",
    "    else:\n",
    "        raise Exception(f\"Error in testing question {idx}, exited with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_COUNT = FINAL.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_CSV = r\"generatedQuestions/valid_questions.csv\"\n",
    "if os.path.exists(VALID_CSV):\n",
    "    FINAL.to_csv(VALID_CSV, mode='a', header=False, index=False)\n",
    "else:\n",
    "    FINAL.to_csv(VALID_CSV, mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(FINAL.shape[0]):\n",
    "    print(f\"{FINAL.iloc[idx].name}: {FINAL.iloc[idx]['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submits the question to the Aurora API\n",
    "submission_count = 0\n",
    "for idx in range(FINAL.shape[0]):\n",
    "    submission = prep_data(FINAL.iloc[idx])\n",
    "    submission = submission.to_json()\n",
    "    response = requests.post(SUBMIT_URL, headers=headers, data=submission)\n",
    "    if response.status_code == 200:\n",
    "        submission_count += 1\n",
    "    else:\n",
    "        raise Exception(f\"Error in submitting question {idx}, exited with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{WORKING_COUNT} questions were generated, {FINAL_COUNT} were valid, and {submission_count} were submitted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
